# ------------------------------------------------------------------------------------
#  Test Results Validation (Reusable Workflow) (GoFortress)
#
#  Purpose: Validate and aggregate test results from all test workflows including
#  matrix tests, fuzz tests, and provide comprehensive failure analysis.
#
#  This workflow handles:
#    - Downloading test result artifacts from all test workflows
#    - Validating test statistics and exit codes
#    - Aggregating failure information across test types
#    - Providing detailed failure analysis with smart filtering
#    - Creating comprehensive validation reports
#
#  Maintainer: @mrz1836
#
# ------------------------------------------------------------------------------------

name: GoFortress (Test Validation)

on:
  workflow_call:
    inputs:
      env-json:
        description: "JSON string of environment variables"
        required: true
        type: string
      primary-runner:
        description: "Primary runner OS"
        required: true
        type: string
      fuzz-testing-enabled:
        description: "Whether fuzz testing is enabled"
        required: true
        type: string

# Security: Restrictive default permissions with job-level overrides for least privilege access
permissions:
  contents: read

jobs:
  # ----------------------------------------------------------------------------------
  # Validate Test Results
  # ----------------------------------------------------------------------------------
  validate-test-results:
    name: 🔍 Validate Test Results
    if: always() # Always run to check results even if jobs continued on error
    permissions:
      contents: read # Read repository content for validation
    runs-on: ${{ inputs.primary-runner }}

    steps:
      # ————————————————————————————————————————————————————————————————
      # Checkout code (required for local actions)
      # ————————————————————————————————————————————————————————————————
      - name: 📥 Checkout code
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0

      # ————————————————————————————————————————————————————————————————
      # Parse environment variables
      # ————————————————————————————————————————————————————————————————
      - name: 🔧 Parse environment variables
        uses: ./.github/actions/parse-env
        with:
          env-json: ${{ inputs.env-json }}

      # ————————————————————————————————————————————————————————————————
      # Download unit test result artifacts with resilience
      # ————————————————————————————————————————————————————————————————
      - name: 📥 Download unit test results (with retry)
        uses: ./.github/actions/download-artifact-resilient
        with:
          pattern: "test-results-unit-*"
          path: test-results/
          merge-multiple: true
          max-retries: ${{ env.ARTIFACT_DOWNLOAD_RETRIES }}
          retry-delay: ${{ env.ARTIFACT_DOWNLOAD_RETRY_DELAY }}
          timeout: ${{ env.ARTIFACT_DOWNLOAD_TIMEOUT }}
          continue-on-error: ${{ env.ARTIFACT_DOWNLOAD_CONTINUE_ON_ERROR }}

      - name: 📥 Download test statistics (with retry)
        uses: ./.github/actions/download-artifact-resilient
        with:
          pattern: "test-stats-*"
          path: test-results/
          merge-multiple: true
          max-retries: ${{ env.ARTIFACT_DOWNLOAD_RETRIES }}
          retry-delay: ${{ env.ARTIFACT_DOWNLOAD_RETRY_DELAY }}
          timeout: ${{ env.ARTIFACT_DOWNLOAD_TIMEOUT }}
          continue-on-error: ${{ env.ARTIFACT_DOWNLOAD_CONTINUE_ON_ERROR }}

      - name: 📥 Download fuzz results (if enabled, with retry)
        if: inputs.fuzz-testing-enabled == 'true'
        uses: ./.github/actions/download-artifact-resilient
        with:
          pattern: "test-results-fuzz-*"
          path: test-results/
          merge-multiple: true
          max-retries: ${{ env.ARTIFACT_DOWNLOAD_RETRIES }}
          retry-delay: ${{ env.ARTIFACT_DOWNLOAD_RETRY_DELAY }}
          timeout: ${{ env.ARTIFACT_DOWNLOAD_TIMEOUT }}
          continue-on-error: ${{ env.ARTIFACT_DOWNLOAD_CONTINUE_ON_ERROR }}

      # ————————————————————————————————————————————————————————————————
      # Flatten downloaded artifacts to expected directory structure
      # ————————————————————————————————————————————————————————————————
      - name: 🗂️ Flatten downloaded artifacts
        if: always()
        run: |
          echo "🗂️ Flattening downloaded artifacts..."

          # Process test statistics (move from subdirectories to test-results/)
          if [ -d "test-results/" ]; then
            echo "📋 Directory structure before flattening:"
            find test-results/ -name "*.json" -type f | head -10 | sed 's/^/  • /' || echo "  No JSON files found"

            find test-results/ -name "*.json" -type f | while read -r file; do
              filename=$(basename "$file")
              echo "Moving $file to test-results/$filename"
              cp "$file" "test-results/$filename"
            done

            echo "📋 Available statistics files after flattening:"
            ls -la test-results/*-stats-*.json 2>/dev/null || echo "  No statistics files found"
          else
            echo "⚠️ No test-results directory found"
          fi

      # ————————————————————————————————————————————————————————————————
      # Enhanced test results validation with failure details
      # ————————————————————————————————————————————————————————————————
      - name: 🔍 Validate test results
        run: |
          echo "🔍 Validating test results from enhanced statistics..."
          VALIDATION_FAILED=false
          TOTAL_FAILURES=0
          TOTAL_TESTS=0

          # Check regular test results
          if compgen -G "test-results/test-stats-*.json" >/dev/null 2>&1; then
            echo "📋 Found test statistics files:"
            ls -la test-results/test-stats-*.json

            for stats_file in test-results/test-stats-*.json; do
              echo "📊 Checking $stats_file..."

              # Extract enhanced test results
              TEST_PASSED=$(jq -r '.test_passed // empty' "$stats_file")
              TEST_EXIT_CODE=$(jq -r '.test_exit_code // empty' "$stats_file")
              TEST_NAME=$(jq -r '.name // empty' "$stats_file")
              TEST_MODE=$(jq -r '.test_mode // "unknown"' "$stats_file")
              SUITE_FAILURES=$(jq -r '.total_failures // 0' "$stats_file")
              AFFECTED_PACKAGES=$(jq -r '.affected_packages // 0' "$stats_file")
              TEST_COUNT=$(jq -r '.test_count // 0' "$stats_file")

              echo "  • Test Suite: $TEST_NAME"
              echo "  • Mode: $TEST_MODE"
              echo "  • Tests: $TEST_COUNT"
              echo "  • Exit Code: $TEST_EXIT_CODE"
              echo "  • Passed: $TEST_PASSED"

              if [[ "$TEST_PASSED" == "false" ]] || [[ "$TEST_EXIT_CODE" != "0" ]]; then
                echo "  • Failed Tests: $SUITE_FAILURES"
                echo "  • Affected Packages: $AFFECTED_PACKAGES"

                # Show specific failure details if available
                FAILURE_DETAILS=$(jq -r '.failure_details // null' "$stats_file")
                if [[ "$FAILURE_DETAILS" != "null" ]] && [[ "$FAILURE_DETAILS" != "[]" ]]; then
                  echo "  • Failed Test Names:"
                  echo "$FAILURE_DETAILS" | jq -r '.[] | "    - \(.Test) (\(.Package | split("/") | .[-1] // .[-2] // .))"' 2>/dev/null | head -5 || true
                fi

                echo "❌ Test suite '$TEST_NAME' failed with exit code $TEST_EXIT_CODE ($SUITE_FAILURES failures)"
                VALIDATION_FAILED=true
                TOTAL_FAILURES=$((TOTAL_FAILURES + SUITE_FAILURES))
              else
                echo "✅ Test suite '$TEST_NAME' passed"
              fi

              TOTAL_TESTS=$((TOTAL_TESTS + TEST_COUNT))
            done
          else
            echo "⚠️ No regular test statistics found"
          fi

          # Check fuzz test results if enabled
          if [[ "${{ inputs.fuzz-testing-enabled }}" == "true" ]]; then
            if compgen -G "test-results/fuzz-stats-*.json" >/dev/null 2>&1; then
              echo "📋 Found fuzz test statistics files:"
              ls -la test-results/fuzz-stats-*.json

              for stats_file in test-results/fuzz-stats-*.json; do
                echo "📊 Checking $stats_file..."

                # Extract fuzz test results
                FUZZ_PASSED=$(jq -r '.test_passed // empty' "$stats_file")
                FUZZ_EXIT_CODE=$(jq -r '.test_exit_code // empty' "$stats_file")
                FUZZ_NAME=$(jq -r '.name // empty' "$stats_file")

                echo "  • Fuzz Test: $FUZZ_NAME"
                echo "  • Exit Code: $FUZZ_EXIT_CODE"
                echo "  • Passed: $FUZZ_PASSED"

                if [[ "$FUZZ_PASSED" == "false" ]] || [[ "$FUZZ_EXIT_CODE" != "0" ]]; then
                  echo "❌ Fuzz test suite '$FUZZ_NAME' failed with exit code $FUZZ_EXIT_CODE"
                  VALIDATION_FAILED=true
                else
                  echo "✅ Fuzz test suite '$FUZZ_NAME' passed"
                fi

                # Add fuzz test count to total
                FUZZ_TEST_COUNT=$(jq -r '.fuzz_test_count // 0' "$stats_file")
                echo "  • Fuzz Tests: $FUZZ_TEST_COUNT"
                TOTAL_TESTS=$((TOTAL_TESTS + FUZZ_TEST_COUNT))
              done
            else
              echo "⚠️ No fuzz test statistics found (fuzz testing was enabled)"
            fi
          fi

          # Enhanced validation summary
          echo ""
          echo "🏁 Validation Summary:"
          echo "  • Total Tests: $TOTAL_TESTS"
          echo "  • Total Failures: $TOTAL_FAILURES"

          # Display detailed failure information if tests failed
          if [[ "$VALIDATION_FAILED" == "true" ]]; then
            echo ""
            echo "🔍 Detailed Failure Analysis:"
            echo "==============================================="

            # Look for failure details in downloaded artifacts
            if compgen -G "test-results/test-failures-summary.json" >/dev/null 2>&1; then
              echo "📋 Found structured failure details:"
              for failure_file in test-results/test-failures-summary.json; do
                echo ""
                echo "📄 Processing: $failure_file"

                # Extract and display package-level failures
                PACKAGES=$(jq -r 'length' "$failure_file" 2>/dev/null || echo "0")
                if [[ "$PACKAGES" -gt 0 ]]; then
                  echo "  • Affected Packages: $PACKAGES"

                  # Show package summary (filter out malformed package names)
                  jq -r '.[] | select(.Package | test("^[a-zA-Z0-9/_.-]+$")) | "  📦 \(.Package | split("/") | .[-1] // .[-2] // .): \(.failures | length) failure(s)"' "$failure_file" 2>/dev/null || true

                  echo ""
                  echo "🚨 Failed Tests:"
                  echo "---------------"

                  # Show detailed failed tests with package context (leaf failures only)
                  #
                  # Problem: Go's nested test structure (TestA/TestB/TestC) reports failures for
                  # all parent tests when a leaf test fails, causing confusing output like:
                  #   ❌ TestNetworkEdgeCases/concurrent_api_operations/concurrency_3 (integration) <- actual failure
                  #   ❌ TestNetworkEdgeCases/concurrent_api_operations (integration)              <- parent (redundant)
                  #   ❌ TestNetworkEdgeCases (integration)                                       <- parent (redundant)
                  #   ❌  (integration)                                                           <- empty (artifact)
                  #
                  # Solution: Extract and deduplicate to show only the actual failed leaf tests
                  # This reduces "4 failures" to "1 actual failure" for better clarity.
                  RAW_FAILED_TESTS=$(jq -r '.[] as $parent | select($parent.Package | test("^[a-zA-Z0-9/_.-]+$")) | $parent.failures[] | .Test + " (" + ($parent.Package | split("/") | .[-1] // .[-2] // .) + ")"' "$failure_file" 2>/dev/null)

                  # Smart filtering: Only show the most specific (deepest nested) test failures
                  FAILED_TESTS=$(echo "$RAW_FAILED_TESTS" | awk '
                    {
                      # Skip empty lines
                      if ($0 == "" || $0 ~ /^[[:space:]]*$/) next

                      # Extract test name before package info
                      if (match($0, /^([^(]*[^[:space:]]) \(.*\)$/)) {
                        testname = substr($0, RSTART, RLENGTH)
                        gsub(/ \(.*\)$/, "", testname)
                        # Remove leading/trailing whitespace
                        gsub(/^[[:space:]]+|[[:space:]]+$/, "", testname)
                        # Skip if testname is empty
                        if (testname == "") next

                        # Count depth by number of "/" characters
                        depth_counter = testname
                        gsub(/[^\/]/, "", depth_counter)
                        depth = length(depth_counter)
                        tests[NR] = $0
                        depths[NR] = depth
                        names[NR] = testname
                      }
                    }
                    END {
                      # For each test, check if there is a more specific (deeper) version
                      for (i in tests) {
                        is_leaf = 1
                        for (j in tests) {
                          if (i != j && depths[j] > depths[i] && index(names[j], names[i]) == 1) {
                            is_leaf = 0
                            break
                          }
                        }
                        if (is_leaf && names[i] != "") print tests[i]
                      }
                    }
                  ' | head -20)

                  if [[ -n "$FAILED_TESTS" ]]; then
                    echo "$FAILED_TESTS" | sed 's/^/  ❌ /'

                    # Update failure count to reflect actual unique failures
                    ACTUAL_UNIQUE_FAILURES=$(echo "$FAILED_TESTS" | grep -v '^[[:space:]]*$' | wc -l)
                    RAW_FAILURE_COUNT=$(echo "$RAW_FAILED_TESTS" | grep -v '^[[:space:]]*$' | wc -l)
                    if [[ $RAW_FAILURE_COUNT -gt $ACTUAL_UNIQUE_FAILURES ]]; then
                      echo ""
                      echo "  📊 Note: Showing $ACTUAL_UNIQUE_FAILURES actual failures"
                      echo "         (filtered from $RAW_FAILURE_COUNT nested test hierarchy entries)"
                    fi
                  else
                    echo "  ⚠️ No test failures found in JSON structure"
                    echo "  📄 Raw JSON content:"
                    head -c 2000 "$failure_file" | sed 's/^/  /'
                  fi

                  # Show any error outputs if available from the enhanced failure details
                  ERROR_OUTPUTS=$(jq -r '.[] as $package | select($package.Package | test("^[a-zA-Z0-9/_.-]+$")) | $package.failures[] | select(.Output and .Output != "" and .Output != null) | "❌ \(.Test) (\($package.Package | split("/") | .[-1] // .[-2] // .))\n\(.Output)\n"' "$failure_file" 2>/dev/null | head -c 3000)
                  if [[ -n "$ERROR_OUTPUTS" ]]; then
                    echo ""
                    echo "📝 Test Error Messages:"
                    echo "----------------------"
                    echo "$ERROR_OUTPUTS"
                  fi
                else
                  echo "  • No structured failure data found in JSON"
                fi
              done

            # Fallback to simple text files if JSON not available
            elif compgen -G "test-results/test-failures.txt" >/dev/null 2>&1; then
              echo "📋 Found text failure details:"
              for failure_file in test-results/test-failures.txt; do
                if [[ -s "$failure_file" ]]; then
                  echo ""
                  echo "📄 Processing: $failure_file"
                  echo "🚨 Failed Tests:"
                  echo "---------------"
                  head -20 "$failure_file" | while IFS= read -r line; do
                    echo "  ❌ $line"
                  done
                fi
              done

            else
              echo "⚠️ No detailed failure information found in downloaded artifacts"
              echo "   Available files:"
              ls -la test-results/ 2>/dev/null | head -10 || echo "   No files found"
            fi

            echo ""
            echo "==============================================="
            echo "❌ Test validation failed - $TOTAL_FAILURES test(s) failed across all suites"
            echo "::error title=Test Validation Failed::$TOTAL_FAILURES test(s) failed across all test suites. Check enhanced failure details above."
            exit 1
          else
            echo "✅ All $TOTAL_TESTS tests passed validation"
          fi

      # ————————————————————————————————————————————————————————————————
      # Create validation summary for GitHub UI
      # ————————————————————————————————————————————————————————————————
      - name: 📊 Create validation summary
        if: always()
        run: |
          echo "## 🔍 Test Validation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count test suites and statistics (using reliable file counting)
          TEST_SUITE_COUNT=0
          if ls test-results/test-stats-*.json >/dev/null 2>&1; then
            TEST_SUITE_COUNT=$(ls test-results/test-stats-*.json | wc -l)
          fi

          FUZZ_SUITE_COUNT=0
          if [[ "${{ inputs.fuzz-testing-enabled }}" == "true" ]]; then
            if ls test-results/fuzz-stats-*.json >/dev/null 2>&1; then
              FUZZ_SUITE_COUNT=$(ls test-results/fuzz-stats-*.json | wc -l)
            fi
          fi

          # Calculate total test counts across all suites
          TOTAL_REGULAR_TESTS=0
          TOTAL_FUZZ_TESTS=0

          # Aggregate regular test counts
          if [[ $TEST_SUITE_COUNT -gt 0 ]]; then
            for stats_file in test-results/test-stats-*.json; do
              if [[ -f "$stats_file" ]]; then
                TEST_COUNT=$(jq -r '.test_count // 0' "$stats_file")
                TOTAL_REGULAR_TESTS=$((TOTAL_REGULAR_TESTS + TEST_COUNT))
              fi
            done
          fi

          # Aggregate fuzz test counts
          if [[ $FUZZ_SUITE_COUNT -gt 0 ]]; then
            for stats_file in test-results/fuzz-stats-*.json; do
              if [[ -f "$stats_file" ]]; then
                FUZZ_COUNT=$(jq -r '.fuzz_test_count // 0' "$stats_file")
                TOTAL_FUZZ_TESTS=$((TOTAL_FUZZ_TESTS + FUZZ_COUNT))
              fi
            done
          fi

          echo "- **Test Matrix Jobs**: $TEST_SUITE_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests**: $TOTAL_REGULAR_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Fuzz Test Jobs**: $FUZZ_SUITE_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Fuzz Tests**: $TOTAL_FUZZ_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Validation Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show per-suite breakdown if statistics available
          if [[ $TEST_SUITE_COUNT -gt 0 ]]; then
            echo "### 📊 Test Suite Breakdown" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            for stats_file in test-results/test-stats-*.json; do
              if [[ -f "$stats_file" ]]; then
                TEST_NAME=$(jq -r '.name // "Unknown"' "$stats_file")
                TEST_PASSED=$(jq -r '.test_passed // false' "$stats_file")
                TEST_COUNT=$(jq -r '.test_count // 0' "$stats_file")
                FAILURES=$(jq -r '.total_failures // 0' "$stats_file")

                if [[ "$TEST_PASSED" == "true" ]]; then
                  echo "- ✅ **$TEST_NAME**: $TEST_COUNT tests passed" >> $GITHUB_STEP_SUMMARY
                else
                  echo "- ❌ **$TEST_NAME**: $FAILURES/$TEST_COUNT tests failed" >> $GITHUB_STEP_SUMMARY
                fi
              fi
            done
          fi

          if [[ $FUZZ_SUITE_COUNT -gt 0 ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 🎯 Fuzz Test Breakdown" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            for stats_file in test-results/fuzz-stats-*.json; do
              if [[ -f "$stats_file" ]]; then
                FUZZ_NAME=$(jq -r '.name // "Unknown"' "$stats_file")
                FUZZ_PASSED=$(jq -r '.test_passed // false' "$stats_file")
                FUZZ_COUNT=$(jq -r '.fuzz_test_count // 0' "$stats_file")

                if [[ "$FUZZ_PASSED" == "true" ]]; then
                  echo "- ✅ **$FUZZ_NAME**: $FUZZ_COUNT fuzz tests passed" >> $GITHUB_STEP_SUMMARY
                else
                  echo "- ❌ **$FUZZ_NAME**: Fuzz tests failed" >> $GITHUB_STEP_SUMMARY
                fi
              fi
            done
          fi

      # ————————————————————————————————————————————————————————————————
      # Upload validation artifacts for completion report
      # ————————————————————————————————————————————————————————————————
      - name: 📤 Upload validation summary
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: validation-summary
          path: test-results/
          retention-days: 1
          if-no-files-found: ignore
